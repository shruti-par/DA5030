---
title: "DA5030.A7.Parpattedar"
author: "Shruti Parpattedar"
date: "March 22, 2019"
output: pdf_document
---

# Problem 1
## Step 1 - Collecting data 
Downloading the concrete dataset.

## Step 2 - Exploring and preparing the data 
Loading and exploring the dataset. 

Next, normalizing the data and creating training and testing datasets.
```{r Question1.2}
concrete <- read.csv("concrete.csv")
str(concrete)

normalize <- function(x) 
{    
  return((x - min(x)) / (max(x) - min(x)))  
} 

concrete_norm <- as.data.frame(lapply(concrete, normalize)) 
summary(concrete_norm$strength)
summary(concrete$strength) 

concrete_train <- concrete_norm[1:773, ] 
concrete_test <- concrete_norm[774:1030, ] 
```

## Step 3 – Training a model on the data 
Training the model on the training dataset using the neuralnet function.

Next, plotting the model.
```{r Question1.3}
library(neuralnet)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water + superplastic + 
                              coarseagg + fineagg + age,    data = concrete_train)

plot(concrete_model)
```

## Step 4 – Evaluating model performance
Testing the model on the testing dataset.

Next, finding the correlation between the predicted strength and original strength.
```{r Question1.4}
model_results <- compute(concrete_model, concrete_test[1:8]) 
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
```

## Step 5 – Improving model performance
Applying the neuralnet function on certain attributes of the data with 5 hidden vertices.

Next, plotting the new model.

Testing the model on the testing dataset.

Next, finding the correlation between the predicted strength and original strength.
```{r Question1.5}
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + 
                               superplastic + coarseagg + fineagg + age,
                             data = concrete_train, hidden = 5) 
plot(concrete_model2)

model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

# Problem 2

## Step 1 - Collecting the data
Downloading the letter dataset.

## Step 2 – Exploring and preparing the data
Loading and exploring the dataset. 

Next, creating training and testing datasets.
```{r Question2.2}
letters <- read.csv("letterdata.csv")
str(letters)

letters_train <- letters[1:16000, ] 
letters_test  <- letters[16001:20000, ]
```

## Step 3 – Training a model on the data
Training the model on the training dataset using the ksvm function with the vanilladot (linear) kernel.
```{r Question2.3}
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train, kernel = "vanilladot")
letter_classifier 
```

## Step 4 – Evaluating model performance 
Testing the model on the testing dataset.

Next, displaying the table with the predicted and original values.
```{r Question2.4}
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)

agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
```

## Step 5 – Improving model performance
Training the model on the training dataset using the ksvm function with the rbfdot (radial basis, Gaussian) kernel.

Testing the model on the testing dataset.

Next, displaying the table with the predicted and original values.
```{r Question2.5}
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)

agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
```

# Problem 3

## Step 1 - Collecting data
Downloading the groceries dataset.

## Step 2 - Exploring and preparing the data 
Loading the groceries dataset and inspect the elements.

Next, displaying the top three item frequencies.
```{r Question3.2}
library(arules)
groceries <- read.transactions("groceries.csv", sep = ",")
summary(groceries)

inspect(groceries[1:5])
itemFrequency(groceries[, 1:3])

# Visualizing item support – item frequency plots 
itemFrequencyPlot(groceries, support = 0.1)
itemFrequencyPlot(groceries, topN = 20)

# Visualizing the transaction data – plotting the sparse matrix 
image(groceries[1:5])
image(sample(groceries, 100))
```

## Step 3 – Training a model on the data
Applying the apriori algorithm to the groceries dataset.

Generating the apriori rules for the given parameters.
```{r Question3.3}
apriori(groceries)

groceryrules <- apriori(groceries, 
                        parameter = list(support = 0.006,
                                         confidence = 0.25, 
                                         minlen = 2))

groceryrules
```

## Step 4 – Evaluating model performance 
Inspecting the grocery rules.
```{r Question3.4}
summary(groceryrules)
inspect(groceryrules[1:3])
```

## Step 5 – Improving model performance
Sorting the set of association rules and then displaying the first 5 rules sorted by their lift values
```{r Question3.5}
inspect(sort(groceryrules, by = "lift")[1:5])
```

## Taking subsets of association rules 
```{r}
berryrules <- subset(groceryrules, items %in% "berries")
inspect(berryrules)
```

## Saving association rules to a file or data frame
```{r}
write(groceryrules, file = "groceryrules.csv",
        sep = ",", quote = TRUE, row.names = FALSE)
groceryrules_df <- as(groceryrules, "data.frame")
str(groceryrules_df)

```

